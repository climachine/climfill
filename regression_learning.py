"""
Copyright 2021 ETH Zurich, contributor: Verena Bessenbacher

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

   http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.

This file gives you the class for updating an initial guess of a 
missing value in a dataset by taking into account the multivariate
dependence structure of the data.
"""

import numpy as np
import copy
from numpy import Inf

class Imputation: 
    """
    A Gapfill procedure for multivariate data.

    An algorithm designed to fill gaps in tabular data by iteratively
    learning to regress one variable (or feature) with all the others. The algorithm needs
    initial estimates provided for the gaps which it uses as a starting point for 
    the iterative procedure. Built upon [1] but with several adaptations that tailor it to 
    the needs of geoscientific datasets (for more details see [2]).

    ...

    Parameters
    ----------
    epsilon: int, default=1e-5
    convergence criteria for the iterative algorithm. The difference in the data between two iterations is defined as the summed absolute difference between the data matrix of the new and old iteration and called delta. If the difference of the delta from the last iteration and the delta from the current one is below epsilon, the algorithm converged and the gapfill results are returned. This is a relaxed version of the convergence criterium in Stekhoven and Buehlmann (2012) [1].

    maxiter: int, default=Inf
    second convergence criterium for the iterative algorithm. Alternatively, even if epsilon is not reached, the algorithm stops after the maximum number of allowed iterations maxiter is reached and the gapfilled data is returned.
    
    miniter: int, default=5
    minimum number of iterations before convergence criterion is checked

    miniter_below: int, default=5
    minimum number of iterations after convergence criterion is checked before iteration is stopped. Only applied if convergence is reached via epsilon, not via maxiter

    References
    ----------
    .. [1] Stekhoven, D. J. and Buehlmann, P. (2012): MissForest -- non-parametric missing value imputation for mixed-type data. Bioinformatics, 28, 1, 112-118.
    .. [2] Bessenbacher, V., Gudmundsson, L., Seneviratne, S. I. (2021): CLIMFILL: A Framework for Intelligently Gap-filling Earth Observations (in prep.)

    Examples
    --------
    # TODO missing: init gapfill, data production

    from regression_learning import Imputation
    import xarray as xr
    import numpy as np

    data = xr.open_dataset('/path/to/gappy/dataset')
    mask = np.isnan(data)

    variables = ['temperature', 'precipitation']
    rf_settings = {'n_estimators': 100,
                  'min_samples_leaf': 2,
                  'max_features': 0.5, 
                  'max_samples': 0.5}
    regr_dict = {variable: RandomForestRegressor(**kwargs) for variable in variables}
    maxiter = 10

    impute = Imputation(maxiter=maxiter)
    imputed_data, fitted_regr_dict = impute.impute(data, mask, regr_dict)

    """

    def __init__(self, epsilon=1e-5, maxiter=np.inf, miniter=5, miniter_below=5):

        self.epsilon = epsilon
        self.maxiter = maxiter # maximal number of total iteration
        self.miniter = miniter # minimal number of total iterations
        self.miniter_below = miniter_below  # minimal number of iterations where epsilon is below threshold

    def _logiter(self, logtrunc):
        """
        Internal function to handle iteration log
        """
        logging.info(f'{logtrunc} new delta: {np.round(self.delta_n, 9)} diff: ' + str(np.round(self.delta_n_old - self.delta_n, 9)) + f' niter: {self.iter} niterbelow: {self.iter_below}')

    def impute(self, data, lostmask, regr_dict, kwargs={}, verbose=1, logtrunc=''):
        """
        Impute (i.e. Gapfill) data by regressing each variable (i.e. column) in data with 
        all other variables in data.

        Parameters
        ----------
        data: xarray dataarray, feature table with rows as datapoints and columns as features
        lostmask: 
            boolean xarray in the same shape as data, indicating which values were originally missing as True and all others as False
        regr_dict: dictionary where the keys are the names of the gappy variables that form a part of the columns in data. variable names must match the coordinate names of data column names. Values are instances of regression functions (e.g. scikit-learn's RandomForestRegressor) that have a fit() and a predict() method.

        kwargs: optional. Some regression functions need additional keywords for the .fit() method. provide these here
    
        verbose: optional, int debug verbosity

        Returns
        ----------
        imputed_data: data of the same shape as input data, where all values that were not missing are still the same and all values that were originally missing are imputed via regression learning

        fitted_regr_dict: dictionary where the variable names are the keys and the fitted regression functions (including all weights) per variable are the values
        """

        if lostmask.sum().values == 0:
            raise ValueError('lostmask does not have any True entry. abort Imputation')

        # define convergence criteria
        self.delta_n = Inf

        # set initial values for iteration params
        self.iter = 0
        self.iter_below = 0

        self.fittedregr = dict()

        varnames = [varname for varname, regr in regr_dict.items()] # within a single run, without changing the dict, the order is not mutated, see https://stackoverflow.com/questions/52268262/does-pythons-dict-items-always-return-the-same-order
        print(varnames)

        # while convergence not reached, loop over features and impute iteratively
        while True:

            # store previously imputed matrix
            data_old = data.copy(deep=True)
            np.array_equal(data, data_old) # keep this otherwise on euler diff gets zero. bug 05032021
            for varname, Regr_orig in regr_dict.items():
                
                # this is very important. some concurrent.futures threads abort because Regr.predict() returns only Infs, because n_estimators within Regr is changed in second variable if Regr is not newly created each iteration. debug took three days uff
                Regr = copy.copy(Regr_orig)

                # divide into predictor and predictands
                if verbose >= 2:
                    logging.info(f'{logtrunc} {varname} divide into predictor and predictands')
                y = data.loc[:,varname]
                notyvars = data.coords['variable'].values.tolist()
                notyvars.remove(varname)
                X = data.loc[:,notyvars]
                y_mask = lostmask.loc[:,varname]

                # fit dimension_reduction
                #if self.unsup is not None:
                #    X = self.unsup.fit_transform(X)

                # divide into missing and not missing values
                if verbose >= 2:
                    logging.info(f'{logtrunc} divide into missing and not missing')
                y_mis = y[y_mask]
                y_obs = y[~y_mask]
                del(y)
                #if self.unsup is not None: # TODO more elegant
                #    X_mis = datasvd[y_mask,:]
                #    X_obs = datasvd[~y_mask,:]
                #else:
                X_mis = X[y_mask.data,:] # fall back on numpy bec variablenams
                X_obs = X[~y_mask.data,:]
                del(X)

                # check if missing values present in subset
                if y_mis.size == 0:
                    if verbose >= 0:
                        logging.info(f'{logtrunc} variable {varname} does not have missing values. skip ...')
                    continue

                # check if enough observations (>2) for fitting present
                if y_obs.size < 2:
                    logging.info(f'{logtrunc} WARNING variable {varname} does not have (enough) observed values. skip ...')
                    continue

                # fit regression
                if verbose >= 2:
                    logging.info(f'{logtrunc} fit to avail obs')
                if kwargs is not None:
                    Regr.fit(X_obs, y_obs, **kwargs)
                else:
                    Regr.fit(X_obs, y_obs)
                self.fittedregr[varname] = Regr

                # predict
                if verbose >= 2:
                    logging.info(f'{logtrunc} predict missing values')
                try:
                    y_predict = Regr.predict(X_mis)
                except RuntimeWarning as e:
                    raise ValueError(f'{logtrunc} {len(Regr.estimators_)}')

                # update estimates for missing values
                if verbose >= 2:
                    logging.info(f'{logtrunc} update estimates for missing values')
                v = np.where(data.coords['variable'] == varname)[0][0] #TODO more elegant
                #print(y_predict)
                #print(data[y_mask,v])
                data[y_mask,v] = y_predict.squeeze()
                del(y_predict) # important! keep

            # calculate stopping criterion
            # troyanskaya svdimpute: change is below 0.01
            # stekhoven & b√ºhlmann missforest: squared norm difference of imputed values
            # increases for the first time ((mod-obs)**2 / mod**2)
            # self-made convergence heuristic: the denominator is default the standard deviation (i think)
            # for standardised data
            # i.e. algorithm converges if change between steps is smaller than epsilon times std
            self.delta_n_old = self.delta_n
            self.delta_n = (np.sum(np.abs(data.loc[:,varnames] - data_old.loc[:,varnames])) / 
                        np.mean(np.abs(data.loc[:,varnames]))).values.item()

            if verbose >= 1:
                self._logiter(logtrunc)

            self.iter += 1 

            # check convergence criteria
            if self.iter > self.maxiter: # reached maximum number of iterations

                if verbose >= -1:
                    self._logiter(f'{logtrunc} TRUNCATED JOB')
                return data_old, self.fittedregr

            elif np.abs(self.delta_n_old - self.delta_n) < self.epsilon and self.iter > self.miniter: # convergence achieved
                self.iter_below += 1

                if self.iter_below > self.miniter_below: # convergence achieved already miniter_below times
                    if verbose >= -1:
                        self._logiter(f'{logtrunc} FINISHED JOB')
                    return data_old, self.fittedregr

            else:
                self.iter_below = 0

if __name__ == '__main__':
    from sklearn.ensemble import RandomForestRegressor
    import xarray as xr
    import numpy as np

    databatch = xr.open_dataset('/path/to/gappy/data/cluster')
    maskbatch = xr.open_dataset('/path/to/mask/data/cluster')

    variables = ['temperature', 'precipitation', 'test', 'test2']
    rf_settings = {'n_estimators': 100,
                  'min_samples_leaf': 2,
                  'max_features': 0.5, 
                  'max_samples': 0.5}
    regr_dict = {variable: RandomForestRegressor(**rf_settings) for variable in variables}
    varnames = [varname for varname, regr in regr_dict.items()] 
    maxiter = 10

    impute = Imputation(maxiter=maxiter)
    imputed_data, fitted_regr_dict = impute.impute(databatch, maskbatch, regr_dict)

    # imputed_data.to_netcdf ...
