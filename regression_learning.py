"""
A PROPER DOCSTRING WILL APPEAR HERE

    @author: Verena Bessenbacher
"""

import numpy as np
import copy
from numpy import Inf

class Imputation: 
    """
    A Gapfill procedure for multivariate data.

    An algorithm designed to fill gaps in tabular data by iteratively
    learning to regress one variable (or feature) with all the others. The algorithm needs
    initial estimates provided for the gaps which it uses as a starting point for 
    the iterative procedure.

    ...

    Parameters
    ----------
    epsilon: int, default=1e-5
    convergence criteria for the iterative algorithm. The difference in the data between two iterations is defined as the summed absolute difference between the data matrix of the new and old iteration and called delta. If the difference of the delta from the last iteration and the delta from the current one is below epsilon, the algorithm converged and the gapfill results are returned. This is a relaxed version of the convergence criterium in Stekhoven and Buehlmann (2012) [1].

    maxiter: int, default=Inf
    second convergence criterium for the iterative algorithm. Alternatively, even if epsilon is not reached, the algorithm stops after the maximum number of allowed iterations maxiter is reached and the gapfilled data is returned.
    
    miniter: int, default=5
    minimum number of iterations before convergence criterion is checked

    miniter_below: int, default=5
    minimum number of iterations after convergence criterion is checked before iteration is stopped. Only applied if convergence is reached via epsilon, not via maxiter

    Methods
    -------
    # TODO add

    Attributes
    ----------
    # TODO add  

    References
    ----------
    .. [1] Stekhoven, D. J. and Buehlmann, P. (2012): MissForest -- non-parametric missing value imputation for mixed-type data. Bioinformatics, 28, 1, 112-118.

    Examples
    --------
    # TODO missing: init gapfill, data production

    from climfill import Climfill
    import xarray as xr
    import numpy as np

    data = xr.open_dataset(largefilepath + f'features_init_label_e{e}f{f}_{initstr}.nc')
    mask = np.isnan(data)

    variables = ['temperature', 'precipitation']
    rf_settings = {'n_estimators': 100,
                  'min_samples_leaf': 2,
                  'max_features': 0.5, 
                  'max_samples': 0.5}
    regr_dict = {variable: RandomForestRegressor(**kwargs) for variable in variables}
    maxiter = 10

    impute = Imputation(maxiter=maxiter)
    imputed_data, fitted_regr_dict = impute.impute(data, mask, regr_dict)

    """

    def __init__(self, epsilon=1e-5, maxiter=np.inf, miniter=5, miniter_below=5):

        self.epsilon = epsilon
        self.maxiter = maxiter # maximal number of total iteration
        self.miniter = miniter # minimal number of total iterations
        self.miniter_below = miniter_below  # minimal number of iterations where epsilon is below threshold

    def _logiter(self, logtrunc):
        """
        Internal function to handle iteration log
        """
        logging.info(f'{logtrunc} new delta: {np.round(self.delta_n, 9)} diff: ' + str(np.round(self.delta_n_old - self.delta_n, 9)) + f' niter: {self.iter} niterbelow: {self.iter_below}')

    def impute(self, data, lostmask, regr_dict, kwargs={}, verbose=1, logtrunc=''):
        """
        Impute (i.e. Gapfill) data by regressing each variable (i.e. column) in data with 
        all other variables in data.

        Parameters
        ----------
        data: xarray dataarray
            feature table (stacked xarray) that can be unstacked along datapoints
        lostmask: 
            xarray in the same shape as data, indicating which values were originally missing as True
        regr_dict: list of variables where estimates need to be updated

        kwargs:
    
        verbose: int

        Returns
        ----------
        """

        if lostmask.sum().values == 0:
            raise ValueError('lostmask does not have any True entry. abort Imputation')

        # define convergence criteria
        self.delta_n = Inf

        # set initial values for iteration params
        self.iter = 0
        self.iter_below = 0

        self.fittedregr = dict()

        varnames = [varname for varname, regr in regr_dict.items()] # within a single run, without changing the dict, the order is not mutated, see https://stackoverflow.com/questions/52268262/does-pythons-dict-items-always-return-the-same-order
        print(varnames)

        # while convergence not reached, loop over features and impute iteratively
        while True:

            # store previously imputed matrix
            data_old = data.copy(deep=True)
            np.array_equal(data, data_old) # keep this otherwise on euler diff gets zero. bug 05032021
            for varname, Regr_orig in regr_dict.items():
                
                # this is very important. some concurrent.futures threads abort because Regr.predict() returns only Infs, because n_estimators within Regr is changed in second variable if Regr is not newly created each iteration. debug took three days uff
                Regr = copy.copy(Regr_orig)

                # divide into predictor and predictands
                if verbose >= 2:
                    logging.info(f'{logtrunc} {varname} divide into predictor and predictands')
                y = data.loc[:,varname]
                notyvars = data.coords['variable'].values.tolist()
                notyvars.remove(varname)
                X = data.loc[:,notyvars]
                y_mask = lostmask.loc[:,varname]

                # fit dimension_reduction
                #if self.unsup is not None:
                #    X = self.unsup.fit_transform(X)

                # divide into missing and not missing values
                if verbose >= 2:
                    logging.info(f'{logtrunc} divide into missing and not missing')
                y_mis = y[y_mask]
                y_obs = y[~y_mask]
                del(y)
                #if self.unsup is not None: # TODO more elegant
                #    X_mis = datasvd[y_mask,:]
                #    X_obs = datasvd[~y_mask,:]
                #else:
                X_mis = X[y_mask.data,:] # fall back on numpy bec variablenams
                X_obs = X[~y_mask.data,:]
                del(X)

                # check if missing values present in subset
                if y_mis.size == 0:
                    if verbose >= 0:
                        logging.info(f'{logtrunc} variable {varname} does not have missing values. skip ...')
                    continue

                # check if enough observations (>2) for fitting present
                if y_obs.size < 2:
                    logging.info(f'{logtrunc} WARNING variable {varname} does not have (enough) observed values. skip ...')
                    continue

                # fit regression
                if verbose >= 2:
                    logging.info(f'{logtrunc} fit to avail obs')
                if kwargs is not None:
                    Regr.fit(X_obs, y_obs, **kwargs)
                else:
                    Regr.fit(X_obs, y_obs)
                self.fittedregr[varname] = Regr

                # predict
                if verbose >= 2:
                    logging.info(f'{logtrunc} predict missing values')
                try:
                    y_predict = Regr.predict(X_mis)
                except RuntimeWarning as e:
                    raise ValueError(f'{logtrunc} {len(Regr.estimators_)}')

                # update estimates for missing values
                if verbose >= 2:
                    logging.info(f'{logtrunc} update estimates for missing values')
                v = np.where(data.coords['variable'] == varname)[0][0] #TODO more elegant
                #print(y_predict)
                #print(data[y_mask,v])
                data[y_mask,v] = y_predict.squeeze()
                del(y_predict) # important! keep

            # calculate stopping criterion
            # troyanskaya svdimpute: change is below 0.01
            # stekhoven & bÃ¼hlmann missforest: squared norm difference of imputed values
            # increases for the first time ((mod-obs)**2 / mod**2)
            # self-made convergence heuristic: the denominator is default the standard deviation (i think)
            # for standardised data
            # i.e. algorithm converges if change between steps is smaller than epsilon times std
            self.delta_n_old = self.delta_n
            self.delta_n = (np.sum(np.abs(data.loc[:,varnames] - data_old.loc[:,varnames])) / 
                        np.mean(np.abs(data.loc[:,varnames]))).values.item()

            if verbose >= 1:
                self._logiter(logtrunc)

            self.iter += 1 

            # check convergence criteria
            if self.iter > self.maxiter: # reached maximum number of iterations

                if verbose >= -1:
                    self._logiter(f'{logtrunc} TRUNCATED JOB')
                return data_old, self.fittedregr

            elif np.abs(self.delta_n_old - self.delta_n) < self.epsilon and self.iter > self.miniter: # convergence achieved
                self.iter_below += 1

                if self.iter_below > self.miniter_below: # convergence achieved already miniter_below times
                    if verbose >= -1:
                        self._logiter(f'{logtrunc} FINISHED JOB')
                    return data_old, self.fittedregr

            else:
                self.iter_below = 0

if __name__ == '__main__':
    from sklearn.ensemble import RandomForestRegressor
    import xarray as xr
    import numpy as np

    databatch = xr.open_dataset('/path/to/gappy/data/cluster')
    maskbatch = xr.open_dataset('/path/to/mask/data/cluster')

    variables = ['temperature', 'precipitation', 'test', 'test2']
    rf_settings = {'n_estimators': 100,
                  'min_samples_leaf': 2,
                  'max_features': 0.5, 
                  'max_samples': 0.5}
    regr_dict = {variable: RandomForestRegressor(**rf_settings) for variable in variables}
    varnames = [varname for varname, regr in regr_dict.items()] 
    maxiter = 10

    impute = Imputation(maxiter=maxiter)
    imputed_data, fitted_regr_dict = impute.impute(databatch, maskbatch, regr_dict)

    # imputed_data.to_netcdf ...
